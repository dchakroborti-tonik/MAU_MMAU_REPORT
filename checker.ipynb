{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Jupyter Notebook Loading Header\n",
    "#\n",
    "# This is a custom loading header for Jupyter Notebooks in Visual Studio Code.\n",
    "# It includes common imports and settings to get you started quickly.\n",
    "\n",
    "# %% [markdown]\n",
    "## Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "path = r'C:\\Users\\DwaipayanChakroborti\\AppData\\Roaming\\gcloud\\legacy_credentials\\dchakroborti@tonikbank.com\\adc.json'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path\n",
    "client = bigquery.Client(project='prj-prod-dataplatform')\n",
    "\n",
    "# %% [markdown]\n",
    "## Configure Settings\n",
    "# Set options or configurations as needed\n",
    "# Example: pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.date(2024, 5, 6)]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def get_week_start_dates(start_date, end_date):\n",
    "    week_start_dates = []\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        week_start_date = current_date - datetime.timedelta(days=current_date.weekday())\n",
    "        week_start_dates.append(week_start_date)\n",
    "        current_date += relativedelta(weeks=1)\n",
    "    return week_start_dates\n",
    "\n",
    "start_date = datetime.date(2024, 5, 6)\n",
    "end_date = datetime.date(2024, 5, 6)\n",
    "\n",
    "week_start_dates = get_week_start_dates(start_date, end_date)\n",
    "print(week_start_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequest",
     "evalue": "400 Name customer_id not found inside MAU at [86:27]; reason: invalidQuery, location: query, message: Name customer_id not found inside MAU at [86:27]\n\nLocation: asia-southeast1\nJob ID: d400039e-60e5-4750-975c-cef66c0930fa\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:189\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\DwaipayanChakroborti\\Myenv\\reporting\\lib\\site-packages\\google\\cloud\\bigquery\\job\\query.py:1937\u001b[0m, in \u001b[0;36mQueryJob.to_dataframe\u001b[1;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, max_results, geography_as_object, bool_dtype, int_dtype, float_dtype, string_dtype, date_dtype, datetime_dtype, time_dtype, timestamp_dtype)\u001b[0m\n\u001b[0;32m   1771\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dataframe\u001b[39m(\n\u001b[0;32m   1772\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1773\u001b[0m     bqstorage_client: Optional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigquery_storage.BigQueryReadClient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1786\u001b[0m     timestamp_dtype: Union[Any, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1787\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas.DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a pandas DataFrame from a QueryJob\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m \n\u001b[0;32m   1790\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1935\u001b[0m \u001b[38;5;124;03m            :mod:`shapely` library cannot be imported.\u001b[39;00m\n\u001b[0;32m   1936\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1937\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[43mwait_for_query\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m query_result\u001b[38;5;241m.\u001b[39mto_dataframe(\n\u001b[0;32m   1939\u001b[0m         bqstorage_client\u001b[38;5;241m=\u001b[39mbqstorage_client,\n\u001b[0;32m   1940\u001b[0m         dtypes\u001b[38;5;241m=\u001b[39mdtypes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1951\u001b[0m         timestamp_dtype\u001b[38;5;241m=\u001b[39mtimestamp_dtype,\n\u001b[0;32m   1952\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\DwaipayanChakroborti\\Myenv\\reporting\\lib\\site-packages\\google\\cloud\\bigquery\\_tqdm_helpers.py:107\u001b[0m, in \u001b[0;36mwait_for_query\u001b[1;34m(query_job, progress_bar_type, max_results)\u001b[0m\n\u001b[0;32m    103\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m get_progress_bar(\n\u001b[0;32m    104\u001b[0m     progress_bar_type, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery is running\u001b[39m\u001b[38;5;124m\"\u001b[39m, default_total, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m )\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m progress_bar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquery_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\DwaipayanChakroborti\\Myenv\\reporting\\lib\\site-packages\\google\\cloud\\bigquery\\job\\query.py:1626\u001b[0m, in \u001b[0;36mQueryJob.result\u001b[1;34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001b[0m\n\u001b[0;32m   1621\u001b[0m     remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1624\u001b[0m     \u001b[38;5;66;03m# Since is_job_done() calls jobs.getQueryResults, which is a\u001b[39;00m\n\u001b[0;32m   1625\u001b[0m     \u001b[38;5;66;03m# long-running API, don't delay the next request at all.\u001b[39;00m\n\u001b[1;32m-> 1626\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_job_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1627\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1629\u001b[0m     \u001b[38;5;66;03m# Use a monotonic clock since we don't actually care about\u001b[39;00m\n\u001b[0;32m   1630\u001b[0m     \u001b[38;5;66;03m# daylight savings or similar, just the elapsed time.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DwaipayanChakroborti\\Myenv\\reporting\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    292\u001b[0m )\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DwaipayanChakroborti\\Myenv\\reporting\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[1;32mc:\\Users\\DwaipayanChakroborti\\Myenv\\reporting\\lib\\site-packages\\google\\api_core\\retry\\retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[0;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    208\u001b[0m         error_list,\n\u001b[0;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    210\u001b[0m         original_timeout,\n\u001b[0;32m    211\u001b[0m     )\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[1;32mc:\\Users\\DwaipayanChakroborti\\Myenv\\reporting\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32mc:\\Users\\DwaipayanChakroborti\\Myenv\\reporting\\lib\\site-packages\\google\\cloud\\bigquery\\job\\query.py:1577\u001b[0m, in \u001b[0;36mQueryJob.result.<locals>.is_job_done\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m job_failed_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1555\u001b[0m     \u001b[38;5;66;03m# Only try to restart the query job if the job failed for\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m     \u001b[38;5;66;03m# a retriable reason. For example, don't restart the query\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1574\u001b[0m     \u001b[38;5;66;03m# into an exception that can be processed by the\u001b[39;00m\n\u001b[0;32m   1575\u001b[0m     \u001b[38;5;66;03m# `job_retry` predicate.\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m     restart_query_job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 1577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m job_failed_exception\n\u001b[0;32m   1578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1579\u001b[0m     \u001b[38;5;66;03m# Make sure that the _query_results are cached so we\u001b[39;00m\n\u001b[0;32m   1580\u001b[0m     \u001b[38;5;66;03m# can return a complete RowIterator.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1586\u001b[0m     \u001b[38;5;66;03m# making any extra API calls if the previous loop\u001b[39;00m\n\u001b[0;32m   1587\u001b[0m     \u001b[38;5;66;03m# iteration fetched the finished job.\u001b[39;00m\n\u001b[0;32m   1588\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reload_query_results(retry\u001b[38;5;241m=\u001b[39mretry, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "\u001b[1;31mBadRequest\u001b[0m: 400 Name customer_id not found inside MAU at [86:27]; reason: invalidQuery, location: query, message: Name customer_id not found inside MAU at [86:27]\n\nLocation: asia-southeast1\nJob ID: d400039e-60e5-4750-975c-cef66c0930fa\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res =  pd.DataFrame()\n",
    "for date in week_start_dates:\n",
    "    query = f\"\"\"with \n",
    "    all_cust   --- To get the total registered customer till the cut off date\n",
    "    AS( \n",
    "        SELECT count(distinct cust_id) Total_registered_users\n",
    "        from prj-prod-dataplatform.dl_customers_db_raw.tdbk_customer_mtb\n",
    "        where created_dt <= \"{date}\" and cust_id is not null \n",
    "        ) ,\n",
    "    open_TSA_cust --- TSA customer which was opened before the cut off date and closed date is either null or greater than cutoff date\n",
    "    AS(\n",
    "        SELECT count (distinct OFCUSTOMERID) Total_open_TSA_customers from `core_raw.customer_accounts` \n",
    "            where CRINTERDESC like 'Transactional Savings Account Inv_R'\n",
    "            and OFCUSTOMERID in (select distinct cust_id \n",
    "            from prj-prod-dataplatform.dl_customers_db_raw.tdbk_customer_mtb where cust_id is not null)\n",
    "            AND OFDATEOPENED <= \"{date}\"\n",
    "            AND (OFDATECLOSED = DATE('1970-01-01') OR  OFDATECLOSED > \"{date}\")\n",
    "            )           \n",
    "            ,\n",
    "    loan_cust --list of customers having loan either in status Normal or Arrears at the cut off date\n",
    "    AS (\n",
    "        SELECT \n",
    "            distinct CAST(lmt.customerId AS STRING) customerId\n",
    "        FROM \n",
    "            `prj-prod-dataplatform.risk_credit_mis.loan_master_table` lmt\n",
    "            inner join prj-prod-dataplatform.risk_credit_mis.loan_bucket_flow_report_core lbfrc on lbfrc.loanAccountNumber = lmt.loanAccountNumber\n",
    "        WHERE lbfrc.bucketDate = date_sub(date(\"{date}\"), interval 1 day)  --- need to change the date\n",
    "        and lbfrc.loanStatus IN ('Normal', 'In Arrears')\n",
    "            )           \n",
    "            ,\n",
    "    ACL_Customer ---- ACL customer are TSBL loans with sub product type FP and disbursement date <= cutoff date\n",
    "        as \n",
    "            (select lmt.customerId FROM `prj-prod-dataplatform.dl_loans_db_raw.tdbk_digital_loan_application` a\n",
    "                inner join `risk_credit_mis.loan_master_table` lmt on lmt.digitalLoanAccountId = a.digitalLoanAccountId\n",
    "                where coalesce(a.loanType, 'NA') = 'TSBL'and coalesce(a.loan_sub_product_type, 'NA') = 'FP'\n",
    "                and date_trunc(a.loanDigitalAppliedDateAndTime, day) <= \"{date}\"\n",
    "                and date_trunc(lmt.disbursementDateTime, day) <= \"{date}\" --- need to change the date\n",
    "                ),\n",
    "    non_ACL_cust --list of all customers who have no ACL loans (loatype TSBL and sub-product type FP)\n",
    "    AS (\n",
    "        SELECT \n",
    "            distinct CAST(customerId AS STRING) customerId\n",
    "        FROM \n",
    "            `prj-prod-dataplatform.risk_credit_mis.loan_master_table`\n",
    "        WHERE \n",
    "            cast(customerId as string) not in (select customerId from loan_cust)\n",
    "    )\n",
    "  \n",
    "    /*So, new definition of MAU is. Customer with open TSA and at least one any transaction for the amount greater than 100 PHP in the last 30 days. \n",
    "    No active loan. OR has balance of the TSA + any Stash greater than 100 PHP. Whoch means if he didnt transact but he has money on TSA - he is MAU. \n",
    "    If he doesnt transact and doesnt have money on TSA but has more than 100 PHP on Stash - he is active */\n",
    "    ,        \n",
    "    MAU_with_trx \n",
    "    AS (\n",
    "        SELECT DISTINCT customer_id \n",
    "        FROM `prj-prod-dataplatform.risk_mart.customer_transactions`\n",
    "        WHERE transaction_date BETWEEN DATE_SUB(date(\"{date}\"), INTERVAL 30 DAY) AND date(\"{date}\")\n",
    "        AND ABS(trx_amount) > 100  --- transaction greater than 100\n",
    "        AND account_type = \"Tonik Account\"  -- TSA account type\n",
    "        AND customer_id NOT IN (SELECT customerId FROM loan_cust)  --- no active loan\n",
    "        )\n",
    "        ,\n",
    "    MAU_with_bal  --- no active loan is not applied to this\n",
    "     AS (\n",
    "        SELECT \n",
    "            DISTINCT client_id \n",
    "        FROM \n",
    "            `prj-prod-dataplatform.risk_mart.customer_balance`\n",
    "        WHERE\n",
    "            clearedbalance > 100  --- balance in any account greater than 100\n",
    "            AND account_type IN (\"Tonik Account\",\"Group Stash\",\"Individual Stash\")  ---- TSA, Group Stash and Individual Stash\n",
    "            AND balanceDateAsOf between date_sub(date(\"{date}\"), interval 30 day) and date(\"{date}\") ---- in last 30 days\n",
    "        )            ,\n",
    "    MAU --- \n",
    "        AS (\n",
    "        SELECT distinct customer_id AS MAU \n",
    "            FROM \n",
    "                (SELECT customer_id \n",
    "                FROM \n",
    "                    MAU_with_trx\n",
    "                UNION DISTINCT\n",
    "                SELECT \n",
    "                    client_id AS customer_id\n",
    "                FROM  MAU_with_bal\n",
    "                )\n",
    "            )\n",
    "    select loanPaidStatus, count(a.customerId), count( MAU.customer_id) from `prj-prod-dataplatform.risk_credit_mis.loan_master_table` a\n",
    "    inner join MAU on MAU.customer_id = cast(a.customerId as string)    \n",
    "    group by 1\n",
    "    \"\"\"\n",
    "            \n",
    "    # /* a) MMAU should be without active SIL, Quck and Flex loan. but when they will have ACL TSA loan - those should be included into MMAUs. \n",
    "    # b)  I would like to ask you to add  \n",
    "    # 1) the active TSA ACL loans issued (today its zero) after the MMAU.  \n",
    "    # 2) Difference between MMAU with TSA aCL loan and TSA ACL LOans that will show us the current pool of eligible customers. \n",
    "    # 3) Offers extended. They should live somewhere on the datalake table. \n",
    "    # For example this week we will issue 1000 offers. Next week we will issue 10 000 more. So netx week it will be 11 k offers sent.   \n",
    "    # And last one 4) Uptake. TSA ACL Loans active divided into Offers sent. as %.   */\n",
    "    # MMAU_customerbase as (select cast(customerId as string)customerId from non_ACL_cust union all (select cast(customerId as string)customerId from ACL_Customer))\n",
    "    # , \n",
    "    # transactional_cust --Mareks criteria\n",
    "    # AS (\n",
    "    #     SELECT \n",
    "    #         DISTINCT customer_id\n",
    "    #     FROM \n",
    "    #         `prj-prod-dataplatform.risk_mart.customer_transactions`\n",
    "    #     WHERE\n",
    "    #         transaction_date BETWEEN DATE_SUB(date(\"{date}\"), INTERVAL 3 MONTH) AND date(\"{date}\")\n",
    "    #         AND customer_id in (SELECT customerId FROM MMAU_customerbase)\n",
    "    #     GROUP BY\n",
    "    #         customer_id\n",
    "    #     HAVING \n",
    "    #         COUNT(*) >= 5\n",
    "    # ),\n",
    "    # cust_with_balance --Mareks criteria\n",
    "    # AS (\n",
    "    #     SELECT \n",
    "    #         DISTINCT client_id AS customer_id \n",
    "    #     FROM \n",
    "    #         `prj-prod-dataplatform.risk_mart.customer_balance`\n",
    "    #     WHERE\n",
    "    #         clearedbalance > 1000\n",
    "    #         AND account_type IN (\"Tonik Account\",\"Group Stash\",\"Individual Stash\")\n",
    "    #         AND balanceDateAsOf between date_sub(date(\"{date}\"), interval 30 day) and date(\"{date}\")\n",
    "    #         AND client_id in (SELECT customerId FROM MMAU_customerbase)\n",
    "    #         ),\n",
    "    #   MMAU\n",
    "    # AS (\n",
    "    #     SELECT \n",
    "    #         COUNT(customer_id) AS MMAU \n",
    "    #     FROM \n",
    "    #         ( SELECT \n",
    "    #         customer_id \n",
    "    #     FROM \n",
    "    #         transactional_cust\n",
    "    #     UNION DISTINCT\n",
    "    #     SELECT \n",
    "    #         customer_id\n",
    "    #     FROM \n",
    "    #         cust_with_balance))\n",
    "    #       ,\n",
    "    # ACL_customers \n",
    "    # AS (\n",
    "    #     SELECT \n",
    "    #   count(distinct lmt.customerId ) ACL_customers\n",
    "    #     FROM \n",
    "    #         `prj-prod-dataplatform.risk_credit_mis.loan_master_table` lmt\n",
    "    #     inner join ACL_Customer ac on ac.customerId = lmt.customerId\n",
    "    #     WHERE \n",
    "    #        date(lmt.disbursementDateTime) between date_sub(date(\"{date}\"), interval 7 day) and date(\"{date}\")),\n",
    "    # Offers_extended \n",
    "    # AS (\n",
    "    #     SELECT count(distinct cust_id) Offers_extended\n",
    "    # FROM \n",
    "    # `prj-prod-dataplatform.dl_loans_db_raw.tdbk_loan_offers_trx` \n",
    "    # where date(offer_start_date) <= date(\"{date}\") \n",
    "    # )\n",
    "    # ,\n",
    "    # b\n",
    "    # AS (\n",
    "    #     SELECT \n",
    "    #         '{date}' AS Date,\n",
    "    #         *\n",
    "    #     FROM \n",
    "    #         all_cust  \n",
    "    #     CROSS JOIN \n",
    "    #         open_TSA_cust\n",
    "    #     CROSS JOIN \n",
    "    #         MAU \n",
    "    #     CROSS JOIN \n",
    "    #         MMAU\n",
    "    #     CROSS JOIN\n",
    "    #         ACL_customers\n",
    "    #     CROSS JOIN \n",
    "    #         Offers_extended)\n",
    "    # select Date,\n",
    "    #         Total_registered_users,\n",
    "    #         Total_open_TSA_customers,\n",
    "    #         MAU,\n",
    "    #         MMAU,\n",
    "    #         ACL_customers,\n",
    "    #         MMAU - ACL_customers  MMAU_ACL_difference, \n",
    "    #         Offers_extended,\n",
    "    #         CASE \n",
    "    #             WHEN Offers_extended<> 0 then ACL_customers/Offers_extended \n",
    "    #             ELSE NULL END AS Uptake\n",
    "            \n",
    "    #         from b;\"\"\"\n",
    "    df = client.query(query).to_dataframe()\n",
    "    # res = pd.concat([res,df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"Maucustomerasofmay62024.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
